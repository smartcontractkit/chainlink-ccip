version: v2beta1

dependencies:
  prometheus:
    path: ${DEPENDENCIES_DIR}/prometheus
    overwriteVars: true
    namespace: ${DEVSPACE_NAMESPACE}
    profiles:
      - add-beholder-config
  grafana:
    path: ${DEPENDENCIES_DIR}/grafana
    overwriteVars: true
    namespace: ${DEVSPACE_NAMESPACE}
    profiles:
      - add-beholder-config
  loki:
    path: ${DEPENDENCIES_DIR}/loki
    overwriteVars: true
    namespace: ${DEVSPACE_NAMESPACE}
    profiles:
      - add-beholder-config
  tempo:
    path: ${DEPENDENCIES_DIR}/tempo
    overwriteVars: true
    namespace: ${DEVSPACE_NAMESPACE}
    profiles:
      - add-beholder-config
  redpanda:
    path: ${DEPENDENCIES_DIR}/redpanda
    overwriteVars: true
    namespace: ${DEVSPACE_NAMESPACE}
    pipeline: beholder

pullSecrets:
  regcred-beholder:
    registry: 804282218731.dkr.ecr.us-west-2.amazonaws.com
    secret: regcred-beholder

pipelines:
  load-test:
    run: |-
      run_dependency_pipelines redpanda
      create_deployments beholder-agent beholder-gateway
      wait_pod --namespace ${DEVSPACE_NAMESPACE} --label-selector app.kubernetes.io/instance=beholder-agent
      wait_pod --namespace ${DEVSPACE_NAMESPACE} --label-selector app.kubernetes.io/instance=beholder-gateway
      create_deployments beholder-load-tester-job
  kind:
    run: |-
      run_dependency_pipelines prometheus grafana loki tempo redpanda
      ensure_pull_secrets regcred-beholder
      create_deployments beholder-client beholder-agent beholder-gateway
  deploy:
    run: |-
      run_dependency_pipelines redpanda
      create_deployments beholder-client beholder-agent beholder-gateway cp-kafka-rest

deployments:
  beholder-client:
    updateImageTags: false
    namespace: ${DEVSPACE_NAMESPACE}
    helm:
      releaseName: beholder-client
      displayOutput: true
      chart:
        name: ${CHAINLINK_HELM_REGISTRY_URI}/generic-app
        version: 1.3.1
      values:
        name: beholder-client
        image:
          repository: 804282218731.dkr.ecr.us-west-2.amazonaws.com/atlas-beholder-test-client
          tag: qa-latest

        replicas: 1

        service:
          port: 9091

        livenessProbe:
          httpGet:
            path: /metrics

        readinessProbe:
          httpGet:
            path: /metrics

        podSecurityContext:
          fsGroup: 999

        args:
          - -otel-exporter-grpc-endpoint
          - beholder-agent:4317

  beholder-load-tester-job:
    kubectl:
      inlineManifest: |-
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: beholder-load-tester
        spec:
          completions: ${NUMBER_OF_LOAD_TESTERS}  # Total number of pods to complete
          parallelism: ${NUMBER_OF_LOAD_TESTERS}  # Number of pods to run simultaneously
          template:
            metadata:
              labels:
                app: beholder-load-tester
            spec:
              containers:
                - name: beholder-load-tester
                  image: 804282218731.dkr.ecr.us-west-2.amazonaws.com/atlas-beholder-load-tester:${LOAD_TESTER_TAG}
                  imagePullPolicy: Always
                  env:
                    - name: OTEL_COLLECTOR_GRPC_ENDPOINT
                      value: "beholder-agent:4317"
                    - name: OTEL_METRICS_READER_INTERVAL
                      value: "${OTEL_METRICS_READER_INTERVAL}"
                    - name: OTEL_TRACE_SAMPLE_RATIO
                      value: "${OTEL_TRACE_SAMPLE_RATIO}"
                    - name: OTEL_GO_X_CARDINALITY_LIMIT
                      value: "${OTEL_GO_X_CARDINALITY_LIMIT}"
                    - name: OTEL_EMITTER_EXPORT_TIMEOUT
                      value: "${OTEL_EMITTER_EXPORT_TIMEOUT}"
                    - name: OTEL_EMITTER_EXPORT_INTERVAL
                      value: "${OTEL_EMITTER_EXPORT_INTERVAL}"
                    - name: OTEL_EMITTER_EXPORT_MAX_BATCH_SIZE
                      value: "${OTEL_EMITTER_EXPORT_MAX_BATCH_SIZE}"
                    - name: OTEL_EMITTER_MAX_QUEUE_SIZE
                      value: "${OTEL_EMITTER_MAX_QUEUE_SIZE}"
                    - name: OTEL_LOG_EXPORT_TIMEOUT
                      value: "${OTEL_LOG_EXPORT_TIMEOUT}"
                    - name: OTEL_LOG_EXPORT_INTERVAL
                      value: "${OTE_LOG_EXPORT_INTERVAL}"
                    - name: OTEL_LOG_EXPORT_MAX_BATCH_SIZE
                      value: "${OTEL_LOG_EXPORT_MAX_BATCH_SIZE}"
                    - name: OTEL_LOG_MAX_QUEUE_SIZE
                      value: "${OTEL-LOG_MAX_QUEUE_SIZE}"
                    - name: METRICS_ENABLED
                      value: "${METRICS_ENABLED}"
                    - name: METRICS_NUMBER
                      value: "${METRICS_NUMBER}"
                    - name: METRICS_GAUGE_ATTRIBUTES_COUNT
                      value: "${METRICS_GAUGE_ATTRIBUTES_COUNT}"
                    - name: METRICS_GAUGE_ATTRIBUTES_POOL_COUNT
                      value: "${METRICS_GAUGE_ATTRIBUTES_POOL_COUNT}"
                    - name: METRICS_BYTES_PER_ATTRIBUTE
                      value: "${METRICS_BYTES_PER_ATTRIBUTE}"
                    - name: TRACES_ENABLED
                      value: "${TRACES_ENABLED}"
                    - name: TRACES_POOL_SIZE
                      value: "${TRACES_POOL_SIZE}"
                    - name: TRACES_TRACE_DEPTH
                      value: "${TRACES_TRACE_DEPTH}"
                    - name: TRACES_ATTRIBUTES_COUNT
                      value: "${TRACES_ATTRIBUTES_COUNT}"
                    - name: TRACES_BYTES_PER_ATTRIBUTE
                      value: "${TRACES_BYTES_PER_ATTRIBUTE}"
                    - name: CUSTOM_MESSAGES_ENABLED
                      value: "${CUSTOM_MESSAGES_ENABLED}"
                    - name: CUSTOM_MESSAGES_PAYLOAD_BYTES
                      value: "${CUSTOM_MESSAGES_PAYLOAD_BYTES}"
                    - name: SCHEDULE_FROM
                      value: "${SCHEDULE_FROM}"
                    - name: SCHEDULE_INCREASE
                      value: "${SCHEDULE_INCREASE}"
                    - name: SCHEDULE_STEPS
                      value: "${SCHEDULE_STEPS}"
                    - name: SCHEDULE_STEPS_DURATION
                      value: "${SCHEDULE_STEPS_DURATION}"
                    - name: SCHEDULE_PLAIN_DURATION
                      value: "${SCHEDULE_PLAIN_DURATION}"
                    - name: LOKI_TENANT_ID
                      value: "${LOKI_TENANT_ID}"
                    - name: LOKI_URL
                      value: "${LOKI_URL}"
                  securityContext:
                    runAsUser: 65534
                    runAsGroup: 65534
                  resources:
                    requests:
                      memory: "2048Mi"
                      cpu: "1500m"
                    limits:
                      memory: "2048Mi"
                      cpu: "1500m"
              restartPolicy: OnFailure
              securityContext:
                fsGroup: 65534
          backoffLimit: 4  # Number of retries before considering the job as failed

  beholder-agent:
    updateImageTags: false
    namespace: ${DEVSPACE_NAMESPACE}
    helm:
      releaseName: beholder-agent
      displayOutput: true
      chart:
        name: opentelemetry-collector
        repo: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.116.0
      values:
        fullnameOverride: beholder-agent
        mode: deployment
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 1001
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL
        image:
          repository: otel/opentelemetry-collector-contrib # Repository from guidelines given to NOPS
          tag: 0.108.0 # Version from guidelines given to NOPS
        ports:
          otlp:
            enabled: true
          otlp-http:
            enabled: true
          jaeger-compact:
            enabled: false
          jaeger-thrift:
            enabled: false
          jaeger-grpc:
            enabled: false
          zipkin:
            enabled: false
          metrics:
            enabled: true
        serviceMonitor:
          enabled: true
          metricsEndpoints:
            - port: metrics
              interval: 1s
          extraLabels:
            release: prometheus
        config:
          receivers:
            otlp:
              protocols:
                grpc:
                  endpoint: 0.0.0.0:4317
                http:
                  endpoint: 0.0.0.0:4318
          exporters:
            debug:
              verbosity: detailed
            otlphttp:
              endpoint: http://beholder-gateway:4318 # port is http
              tls:
                insecure: true
          service:
            telemetry:
              logs:
                level: "debug"
            pipelines:
              traces:
                receivers: [otlp]
                exporters: [debug, otlphttp]
              metrics:
                receivers: [otlp]
                exporters: [debug, otlphttp]
              logs:
                receivers: [otlp]
                exporters: [debug, otlphttp]
        ingress:
          enabled: true
          ingressClassName: nginx-internal
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: HTTP
            external-dns.alpha.kubernetes.io/ttl: "120"
          hosts:
            - host: ${DEVSPACE_NAMESPACE}-beholder-agent-http.${DEVSPACE_INGRESS_BASE_DOMAIN}
              paths:
                - path: /
                  port: 4318
                  pathType: Prefix

  beholder-gateway:
    updateImageTags: false
    namespace: ${DEVSPACE_NAMESPACE}
    helm:
      releaseName: beholder-gateway
      displayOutput: true
      chart:
        name: opentelemetry-collector
        repo: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.116.0
      values:
        fullnameOverride: beholder-gateway
        mode: deployment
        image:
          repository: 804282218731.dkr.ecr.us-west-2.amazonaws.com/atlas-beholder-gateway-otel-collector
          tag: qa-latest
        podSecurityContext:
          fsGroup: 2000
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
        ports:
          otlp:
            enabled: true
          otlp-http:
            enabled: true
          jaeger-compact:
            enabled: false
          jaeger-thrift:
            enabled: false
          jaeger-grpc:
            enabled: false
          zipkin:
            enabled: false
          metrics:
            enabled: true
        serviceMonitor:
          enabled: true
          metricsEndpoints:
            - port: metrics
        # When enabled, the chart will set the GOMEMLIMIT env var to 80% of the configured resources.limits.memory.
        # If no resources.limits.memory are defined then enabling does nothing.
        # It is HIGHLY recommend to enable this setting and set a value for resources.limits.memory.
        useGOMEMLIMIT: true
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 256Mi
        ingress:
          enabled: false
        podDisruptionBudget:
          minAvailable: 2
          maxUnavailable: 1
        autoscaling:
          enabled: false
          minReplicas: 3
          maxReplicas: 6
          targetCPUUtilizationPercentage: 70
          targetMemoryUtilizationPercentage: 60
        extraEnvs:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: KAFKA_SASL_MECHANISM
            value: PLAIN
        config:
          exporters:
            debug:
              verbosity: detailed
            debug/noop:
              verbosity: normal
              sampling_initial: 0
              sampling_thereafter: 0
            beholder_kafka:
              brokers:
                - redpanda:9093
              kafka_sasl_username: ""
              kafka_sasl_password: ""
              schema_registry_url: http://redpanda:8081
              schema_registry_username: ""
              schema_registry_password: ""
          extensions:
            # The health_check extension is mandatory for this chart.
            # Without the health_check extension the collector will fail the readiness and liveliness probes.
            # The health_check extension can be modified, but should never be removed.
            health_check:
              endpoint: ${env:MY_POD_IP}:13133
          processors:
            batch:
              send_batch_size: 5000
              timeout: 10s
            # Default memory limiter configuration for the collector based on k8s resource limits.
            memory_limiter:
              # check_interval is the time between measurements of memory usage.
              check_interval: 5s
              # By default limit_mib is set to 80% of ".Values.resources.limits.memory"
              limit_percentage: 80
              # By default spike_limit_mib is set to 25% of ".Values.resources.limits.memory"
              spike_limit_percentage: 25
            tail_sampling:
              decision_wait: 10s
              expected_new_traces_per_sec: 10
              num_traces: 1000
              policies:
                - name: a-status-policy
                  status_code:
                    status_codes:
                      - ERROR
                  type: status_code
                - name: probabilistic-policy
                  probabilistic:
                    sampling_percentage: 10
                  type: probabilistic
            # prevent empty attribute values from being rejected by thanos
            transform/emptyvalues: # see https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#replace_all_patterns
              error_mode: ignore
              metric_statements:
                - context: datapoint # for 'regular' attributes
                  statements:
                    - replace_all_patterns(attributes, "value", "^$", "NOT_SET")
                - context: resource # for resource attributes
                  statements:
                    - replace_all_patterns(attributes, "value", "^$", "NOT_SET")
          receivers:
            jaeger: null
            prometheus: null
            zipkin: null
            otlp:
              protocols:
                grpc:
                  endpoint: ${env:MY_POD_IP}:4317
                http:
                  endpoint: ${env:MY_POD_IP}:4318
          service:
            telemetry:
              metrics:
                address: ${env:MY_POD_IP}:8888
                level: detailed
            extensions:
              - health_check
            pipelines:
              logs:
                exporters:
                  - debug
                  - beholder_kafka
                processors:
                  - memory_limiter
                  - batch
                receivers:
                  - otlp
              metrics:
                exporters:
                  - debug
                processors:
                  - memory_limiter
                  - transform/emptyvalues
                  - batch
                receivers:
                  - otlp
              traces:
                exporters:
                  - debug/noop
                processors:
                  - memory_limiter
                  - batch
                receivers:
                  - otlp

  cp-kafka-rest:
    namespace: ${DEVSPACE_NAMESPACE}
    helm:
      chart:
        name: component-chart
        repo: https://charts.devspace.sh
      values:
        containers:
          - image: confluentinc/cp-kafka-rest:6.1.0
            env:
              - name: KAFKA_REST_DEBUG
                value: "true"
              - name: KAFKA_REST_HOST_NAME
                value: "cp-kafka-rest"
              - name: KAFKA_REST_BOOTSTRAP_SERVERS
                value: "PLAINTEXT://redpanda:9093"
              - name: KAFKA_REST_LISTENERS
                value: "http://0.0.0.0:8082"
              - name: KAFKA_REST_CLIENT_SECURITY_PROTOCOL
                value: "PLAINTEXT"
              - name: KAFKA_REST_SCHEMA_REGISTRY_URL
                value: "http://redpanda:8081"
            resources:
              limits:
                memory: '400M'
                cpu: '0.2'
        service:
          ports:
            - port: 8082
        podSecurityContext:
          fsGroup: 999
        securityContext:
          runAsUser: 999
          runAsGroup: 999
          fsGroup: 999

profiles:
  - name: kind
    merge:
      deployments:
        beholder-client:
          helm:
            values:
              beholder-client:
                affinity: []
                topologySpreadConstraints: []
                imagePullSecrets:
                  - name: regcred-beholder

  - name: load-test
    merge:
      deployments:
        beholder-agent:
          helm:
            values:
              config:
                exporters:
                  debug:
                    verbosity: normal
                service:
                  pipelines:
                    traces:
                      receivers: [otlp]
                      exporters: [otlphttp]
                    metrics:
                      receivers: [otlp]
                      exporters: [otlphttp]
                    logs:
                      receivers: [otlp]
                      exporters: [otlphttp]
              resources:
                requests:
                  memory: "3Gi"
                  cpu: "4000m"
                limits:
                  memory: "3.5Gi"
                  cpu: "4500m"
        beholder-gateway:
          helm:
            values:
              resources:
                limits:
                  cpu: '1'
                  memory: 512Mi
                requests:
                  cpu: '1'
                  memory: 512Mi
    patches:
      - op: add
        path: vars
        value:
          NUMBER_OF_LOAD_TESTERS:
            default: "1"
          LOAD_TESTER_TAG:
            default: "qa-latest"
          OTEL_METRICS_READER_INTERVAL:
            default: "1s"
          OTEL_TRACE_SAMPLE_RATIO:
            default: "1.0"
          OTEL_EMITTER_EXPORT_TIMEOUT:
            default: "5s"
          OTEL_EMITTER_EXPORT_INTERVAL:
            default: "1s"
          OTEL_EMITTER_EXPORT_MAX_BATCH_SIZE:
            default: "512"
          OTEL_EMITTER_MAX_QUEUE_SIZE:
            default: "2048"
          OTEL_LOG_EXPORT_TIMEOUT:
            default: "30s"
          OTEL_LOG_EXPORT_INTERVAL:
            default: "1s"
          OTEL_LOG_EXPORT_MAX_BATCH_SIZE:
            default: "512"
          OTEL_LOG_MAX_QUEUE_SIZE:
            default: "2048"
          METRICS_ENABLED:
            default: "true"
          METRICS_NUMBER:
            default: "1"
          METRICS_GAUGE_ATTRIBUTES_COUNT:
            default: "5"
          METRICS_GAUGE_ATTRIBUTES_POOL_COUNT:
            default: "20"
          METRICS_BYTES_PER_ATTRIBUTE:
            default: "15"
          OTEL_GO_X_CARDINALITY_LIMIT:
            default: "50"
          TRACES_ENABLED:
            default: "true"
          TRACES_POOL_SIZE:
            default: "20"
          TRACES_TRACE_DEPTH:
            default: "5"
          TRACES_ATTRIBUTES_COUNT:
            default: "5"
          TRACES_BYTES_PER_ATTRIBUTE:
            default: "15"
          CUSTOM_MESSAGES_ENABLED:
            default: "true"
          CUSTOM_MESSAGES_PAYLOAD_BYTES:
            default: "100"
          SCHEDULE_FROM:
            default: "1"
          SCHEDULE_INCREASE:
            default: "5"
          SCHEDULE_STEPS:
            default: "100"
          SCHEDULE_STEPS_DURATION:
            default: "4m"
          SCHEDULE_PLAIN_DURATION:
            default: "3m"
          LOKI_TENANT_ID:
            default: ""
          LOKI_URL:
            default: ""
      - op: add
        path: dependencies.redpanda.profiles
        value:
          - load-test
